{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2YcC7X+sIzzBQ2p+c0O9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohamtalukdar/Enron-Email-Analysis/blob/main/Vlabs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtE8NqFIVqCh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/drive/MyDrive/maildir/\"\n",
        "!tar -xzvf \"/content/drive/MyDrive/enron_with_categories.tar.gz\" -C \"/content/drive/MyDrive/maildir/\""
      ],
      "metadata": {
        "id": "zjub9pFfV9Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk.stem as stemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import gensim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "id": "DMND11GOR5J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Map folder names to label values\n",
        "labels = {\n",
        "    \"1\": \"Company Business, Strategy, etc.\",\n",
        "    \"2\": \"Purely Personal\",\n",
        "    \"3\": \"Personal but in professional context\",\n",
        "    \"4\": \"Logistic Arrangements\",\n",
        "    \"5\": \"Status arrangements\",\n",
        "    \"6\": \"Document editing/checking\",\n",
        "    \"7\": \"Empty message (due to missing attachment)\",\n",
        "    \"8\": \"Empty message\"\n",
        "}\n",
        "\n",
        "root_directory = '/content/drive/MyDrive/maildir/enron_with_categories/'\n",
        "rows = []\n",
        "\n",
        "for folder_name in os.listdir(root_directory):\n",
        "    folder_path = os.path.join(root_directory, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        label = labels[folder_name]\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                with open(os.path.join(folder_path, filename), 'r') as f:\n",
        "                    data = f.read()\n",
        "                    message_body = data.split(\"\\n\\n\")[-1]\n",
        "                    number = int(filename.split(\".\")[0])\n",
        "                    rows.append({\"#\": number,\"Label\": label,\"Message\": message_body})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Clean the email message\n",
        "df['Message'] = df['Message'].apply(lambda x: re.sub(r'[^\\w\\s]|\\d', '', x).lower())\n",
        "df['Message'] = df['Message'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n",
        "df['Message'] = df['Message'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "df['Message'] = df['Message'].apply(lambda x: \" \".join([word for word in word_tokenize(x) if word.isalpha() and word not in set(stopwords.words(\"english\"))]))\n",
        "df['Message'] = df['Message'].apply(lambda x: \" \".join([SnowballStemmer(\"english\").stem(word) for word in x.split()]))\n",
        "\n",
        "# Drop rows with \"Empty message (due to missing attachment)\" or \"Empty message\" labels\n",
        "df = df[df['Label'] != 'Empty message (due to missing attachment)']\n",
        "df = df[df['Label'] != 'Empty message']\n",
        "\n",
        "# Drop rows with empty Message column\n",
        "df = df.dropna(subset=['Message'])\n",
        "df = df[df['Message'] != '']\n",
        "\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "Em1e6pSxhhRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XOGAMFtrTBak",
        "outputId": "77b55111-6f00-4e39-bd37-908996db2190"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           #                             Label  \\\n",
              "1      54659                   Purely Personal   \n",
              "6       9176                   Purely Personal   \n",
              "7     174265                   Purely Personal   \n",
              "8      54544                   Purely Personal   \n",
              "9      54545                   Purely Personal   \n",
              "...      ...                               ...   \n",
              "1696  123875  Company Business, Strategy, etc.   \n",
              "1697  136198  Company Business, Strategy, etc.   \n",
              "1698    9237  Company Business, Strategy, etc.   \n",
              "1699  136205  Company Business, Strategy, etc.   \n",
              "1700  136479  Company Business, Strategy, etc.   \n",
              "\n",
              "                                                Message  \n",
              "1     [hormon, hostag, know, day, month, man, open, ...  \n",
              "6     [call, nbcs, cant, see, tv, cheap, rate, ploy,...  \n",
              "7             [test, note, capabl, enron, home, comput]  \n",
              "8                                         [kevin, moor]  \n",
              "9     [spoke, friend, ibm, san, jose, offic, ibm, fe...  \n",
              "...                                                 ...  \n",
              "1696                 [sue, mara, enron, corp, tel, fax]  \n",
              "1697  [california, chang, stanc, refund, two, side, ...  \n",
              "1698                                             [dave]  \n",
              "1699  [note, lack, agreement, reduc, exposur, mark, ...  \n",
              "1700  [linda, ena, develop, sever, convers, congress...  \n",
              "\n",
              "[875 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11403d5f-ab33-46e2-a1f1-286b47ba8c33\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#</th>\n",
              "      <th>Label</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>54659</td>\n",
              "      <td>Purely Personal</td>\n",
              "      <td>[hormon, hostag, know, day, month, man, open, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9176</td>\n",
              "      <td>Purely Personal</td>\n",
              "      <td>[call, nbcs, cant, see, tv, cheap, rate, ploy,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>174265</td>\n",
              "      <td>Purely Personal</td>\n",
              "      <td>[test, note, capabl, enron, home, comput]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>54544</td>\n",
              "      <td>Purely Personal</td>\n",
              "      <td>[kevin, moor]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>54545</td>\n",
              "      <td>Purely Personal</td>\n",
              "      <td>[spoke, friend, ibm, san, jose, offic, ibm, fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1696</th>\n",
              "      <td>123875</td>\n",
              "      <td>Company Business, Strategy, etc.</td>\n",
              "      <td>[sue, mara, enron, corp, tel, fax]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>136198</td>\n",
              "      <td>Company Business, Strategy, etc.</td>\n",
              "      <td>[california, chang, stanc, refund, two, side, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1698</th>\n",
              "      <td>9237</td>\n",
              "      <td>Company Business, Strategy, etc.</td>\n",
              "      <td>[dave]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>136205</td>\n",
              "      <td>Company Business, Strategy, etc.</td>\n",
              "      <td>[note, lack, agreement, reduc, exposur, mark, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1700</th>\n",
              "      <td>136479</td>\n",
              "      <td>Company Business, Strategy, etc.</td>\n",
              "      <td>[linda, ena, develop, sever, convers, congress...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>875 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11403d5f-ab33-46e2-a1f1-286b47ba8c33')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11403d5f-ab33-46e2-a1f1-286b47ba8c33 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11403d5f-ab33-46e2-a1f1-286b47ba8c33');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = TfidfVectorizer()\n",
        "# X = vectorizer.fit_transform(df['Message'])\n",
        "\n",
        "# scaler = StandardScaler(with_mean=False)\n",
        "# X = scaler.fit_transform(X)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, df['Label'], test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "05nPhSvbzTX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Naive Bayes ###\n",
        "\n",
        "# model = MultinomialNB()\n",
        "# model.fit(X_train, y_train)\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "GE9XpZkpzd1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Random Forest ###\n",
        "\n",
        "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# model.fit(X_train, y_train)\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "H7NilOWt0UH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Support Vector Machine ###\n",
        "\n",
        "# model = SVC(kernel='linear', C=1, random_state=42)\n",
        "# model.fit(X_train, y_train)\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "xUjK11qxbSNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Word2Vec #####\n",
        "\n",
        "\n",
        "# # Get the messages as a list of lists of words\n",
        "# messages = df['Message'].apply(lambda x: x.split()).tolist()\n",
        "# # Train the CBOW word2vec model\n",
        "# model = Word2Vec(messages, sg=1, window=5, min_count=1, negative=10, seed=0)\n",
        "\n",
        "# # Split the data into train and test sets\n",
        "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)\n",
        "\n",
        "# # Get the train and test labels and messages\n",
        "# train_labels = train_df['Label'].tolist()\n",
        "# train_messages = train_df['Message'].apply(lambda x: x.split()).tolist()\n",
        "# test_labels = test_df['Label'].tolist()\n",
        "# test_messages = test_df['Message'].apply(lambda x: x.split()).tolist()\n",
        "\n",
        "# # Convert the messages to average word vectors\n",
        "# train_vectors = np.zeros((len(train_messages), 100))\n",
        "# for i, message in enumerate(train_messages):\n",
        "#     vectors = np.zeros((100,))\n",
        "#     for word in message:\n",
        "#         if word in model.wv:\n",
        "#             vectors += model.wv[word]\n",
        "#     vectors = vectors / len(message)\n",
        "#     train_vectors[i] = vectors\n",
        "\n",
        "# test_vectors = np.zeros((len(test_messages), 100))\n",
        "# for i, message in enumerate(test_messages):\n",
        "#     vectors = np.zeros((100,))\n",
        "#     for word in message:\n",
        "#         if word in model.wv:\n",
        "#             vectors += model.wv[word]\n",
        "#     vectors = vectors / len(message)\n",
        "#     test_vectors[i] = vectors\n",
        "\n",
        "# # Train a classifier on the train data\n",
        "# classifier = SVC(kernel='linear', C=1)\n",
        "# classifier.fit(train_vectors, train_labels)\n",
        "\n",
        "# # Predict the labels for the test data\n",
        "# predictions = classifier.predict(test_vectors)\n",
        "\n",
        "# # Evaluate the model using metrics such as accuracy, precision, recall, and F1-score\n",
        "# accuracy = accuracy_score(test_labels, predictions)\n",
        "# precision = precision_score(test_labels, predictions, average='weighted', zero_division=0)\n",
        "# recall = recall_score(test_labels, predictions, average='weighted')\n",
        "# #f1_score = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "# print(\"Precision:\", precision)\n",
        "# print(\"Recall:\", recall)\n",
        "# #print(\"F1-Score:\", f1_score)\n"
      ],
      "metadata": {
        "id": "P1PS8Wwy4WTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install transformers\n",
        "# from transformers import BertTokenizer\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import transformers\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# df['Message'] = df['Message'].apply(lambda x: tokenizer.encode(str(x), add_special_tokens=True, max_length=512, padding='max_length', truncation=True))\n",
        "\n",
        "# max_len = df['Message'].apply(lambda x: len(x)).max()\n",
        "# df['Message'] = df['Message'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant'))\n",
        "# df['Label'] = df['Label'].map({label: i for i, label in enumerate(labels.values())})\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_data, validation_data, train_labels, validation_labels = train_test_split(df['Message'].tolist(), df['Label'].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "# train_data = torch.tensor(train_data)\n",
        "# validation_data = torch.tensor(validation_data)\n",
        "# train_labels = torch.tensor(train_labels)\n",
        "# validation_labels = torch.tensor(validation_labels)\n",
        "# model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "# classifier = nn.Linear(model.config.hidden_size, len(labels))\n",
        "# model.classifier = classifier\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "# epochs = 10\n",
        "# for epoch in range(1, epochs + 1):\n",
        "#     model.train()\n",
        "#     running_loss = 0\n",
        "#     for i, data in enumerate(zip(train_data, train_labels)):\n",
        "#         optimizer.zero_grad()\n",
        "#         inputs, label = data\n",
        "#         outputs = model(inputs.unsqueeze(0))[0]\n",
        "#         loss = criterion(outputs.view(-1, len(labels)), label.unsqueeze(0).long())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#     if epoch % 5 == 0:\n",
        "#         print('Epoch: {}/{}'.format(epoch, epochs), 'Loss: {:.4f}'.format(running_loss / len(train_data)))"
      ],
      "metadata": {
        "id": "FcEPTj3nRiUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['Message'] = df['Message'].apply(lambda x: x.split())\n",
        "\n",
        "# word_index = {}\n",
        "# index = 0\n",
        "# for message in df['Message']:\n",
        "#     for word in message:\n",
        "#         if word not in word_index:\n",
        "#             word_index[word] = index\n",
        "#             index += 1\n",
        "\n",
        "\n",
        "# co_occurrence_matrix = np.zeros((len(word_index), len(word_index)))\n",
        "# for message in df['Message']:\n",
        "#     for i in range(len(message)):\n",
        "#         for j in range(i + 1, len(message)):\n",
        "#             co_occurrence_matrix[word_index[message[i]]][word_index[message[j]]] += 1\n",
        "#             co_occurrence_matrix[word_index[message[j]]][word_index[message[i]]] += 1\n",
        "\n",
        "\n",
        "# embedding_dim = 100\n",
        "# X = np.zeros((len(word_index), embedding_dim))\n",
        "# epochs = 30\n",
        "# alpha = 0.75\n",
        "# x_max = 100\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     for i in range(len(word_index)):\n",
        "#         for j in range(i + 1, len(word_index)):\n",
        "#             if co_occurrence_matrix[i][j] == 0:\n",
        "#                 continue\n",
        "#             weight = (co_occurrence_matrix[i][j] / x_max) ** alpha\n",
        "#             x = X[i]\n",
        "#             y = X[j]\n",
        "#             d = x - y\n",
        "#             cost = weight * d\n",
        "#             X[i] = x + cost * 0.05\n",
        "#             X[j] = y - cost * 0.05\n",
        "\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def get_cooccurrence_matrix(corpus, vocab_size, window_size=10):\n",
        "#     corpus = [word for message in corpus for word in message.split()]\n",
        "#     word2idx = {word: i for i, word in enumerate(set(corpus))}\n",
        "#     idx2word = {i: word for i, word in enumerate(set(corpus))}\n",
        "    \n",
        "#     cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
        "    \n",
        "#     for i, word in tqdm(enumerate(corpus), total=len(corpus)):\n",
        "#         word_idx = word2idx[word]\n",
        "#         for j in range(max(0, i - window_size), min(len(corpus), i + window_size + 1)):\n",
        "#             if i != j:\n",
        "#                 context_word = corpus[j]\n",
        "#                 context_word_idx = word2idx[context_word]\n",
        "#                 cooccurrence_matrix[word_idx][context_word_idx] += 1\n",
        "                \n",
        "#     return cooccurrence_matrix, word2idx, idx2word\n",
        "\n",
        "# def get_embeddings(cooccurrence_matrix, word2idx, idx2word, embedding_size=100):\n",
        "#     vocab_size = cooccurrence_matrix.shape[0]\n",
        "#     X = np.random.randn(vocab_size, embedding_size)\n",
        "#     Y = np.random.randn(vocab_size, embedding_size)\n",
        "    \n",
        "#     epochs = 100\n",
        "#     learning_rate = 0.01\n",
        "    \n",
        "#     for epoch in range(epochs):\n",
        "#         for i in tqdm(range(vocab_size), total=vocab_size):\n",
        "#             word_embedding = X[i, :]\n",
        "#             for j in range(vocab_size):\n",
        "#                 context_word_embedding = Y[j, :]\n",
        "#                 cooccurrence = cooccurrence_matrix[i][j]\n",
        "                \n",
        "#                 if cooccurrence != 0:\n",
        "#                     weight = (cooccurrence / np.sum(cooccurrence_matrix[i, :]))**0.75\n",
        "#                     diff = word_embedding - context_word_embedding\n",
        "#                     X[i, :] -= learning_rate * weight * diff\n",
        "#                     Y[j, :] += learning_rate * weight * diff\n",
        "                    \n",
        "#     embeddings = {idx2word[i]: X[i, :] for i in range(vocab_size)}\n",
        "    \n",
        "#     return embeddings\n",
        "\n",
        "# corpus = df['Message'].values.tolist()\n",
        "# cooccurrence_matrix, word2idx, idx2word = get_cooccurrence_matrix(corpus, 5000)\n",
        "# embeddings = get_embeddings(cooccurrence_matrix, word2idx, idx2word)"
      ],
      "metadata": {
        "id": "J1hfNAB8lZG-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzT2LS_CrX4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}